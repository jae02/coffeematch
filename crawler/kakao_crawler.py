import asyncio
import json
import os
from typing import List, Optional
from dataclasses import asdict
from common import BasePlatformCrawler, AntiBot, CafeData, ReviewData

class KakaoMapCrawler(BasePlatformCrawler):
    """ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ëŸ¬"""
    
    BASE_URL = "https://map.kakao.com"
    
    async def discover_new_cafes(self, region: str, keyword: str = "ì¹´í˜", limit: int = 10) -> List[CafeData]:
        """ì‹ ê·œ ì¹´í˜ íƒìƒ‰"""
        await self.init_browser()
        page = await self.context.new_page()
        cafes = []
        
        try:
            await page.goto(self.BASE_URL)
            await asyncio.sleep(3)
            
            # ëª¨ë‹¬ì´ë‚˜ dimmed layerê°€ ìˆìœ¼ë©´ ì œê±°
            try:
                await page.evaluate("""
                    const dimmed = document.getElementById('dimmedLayer');
                    if (dimmed) dimmed.style.display = 'none';
                """)
            except:
                pass
            
            # ê²€ìƒ‰ì–´ ì…ë ¥ - ë” ê°„ë‹¨í•œ ë°©ë²• ì‚¬ìš©
            search_query = f"{region} {keyword}"
            try:
                # ê²€ìƒ‰ì°½ì— ì§ì ‘ ê°’ ì„¤ì •
                await page.fill('input#search\\.keyword\\.query', search_query)
                await asyncio.sleep(1)
                await page.click('button#search\\.keyword\\.submit')
                await asyncio.sleep(4)
            except Exception as e:
                print(f"Search failed: {e}")
                # ëŒ€ì²´ ë°©ë²• ì‹œë„
                await page.evaluate(f"""
                    document.querySelector('input#search\\\\.keyword\\\\.query').value = '{search_query}';
                    document.querySelector('button#search\\\\.keyword\\\\.submit').click();
                """)
                await asyncio.sleep(4)
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¹´í˜ ì •ë³´ ì¶”ì¶œ
            cafe_items = await page.query_selector_all('#info\\.search\\.place\\.list > li.PlaceItem')
            
            for idx, item in enumerate(cafe_items[:limit]):
                try:
                    # ì¹´í˜ ì •ë³´ ì¶”ì¶œ
                    name_elem = await item.query_selector('.head_item .tit_name')
                    name = await name_elem.inner_text() if name_elem else ""
                    
                    # ì´ë¦„ ì•ì˜ ì•ŒíŒŒë²³ ì ‘ë‘ì‚¬ ì œê±° (ì˜ˆ: "A ì¹´í˜ì´ë¦„" -> "ì¹´í˜ì´ë¦„")
                    import re
                    name = re.sub(r'^[A-Z]\s+', '', name)
                    
                    # ìƒì„¸ í˜ì´ì§€ ë§í¬ì—ì„œ ID ì¶”ì¶œ
                    link_elem = await item.query_selector('.moreview')
                    detail_url = await link_elem.get_attribute('href') if link_elem else ""
                    platform_id = detail_url.split('/')[-1] if detail_url else f"kakao_{idx}"
                    
                    # ì£¼ì†Œ
                    addr_elem = await item.query_selector('.addr p')
                    address = await addr_elem.inner_text() if addr_elem else ""
                    
                    # ì¹´í…Œê³ ë¦¬
                    category_elem = await item.query_selector('.subcategory')
                    category = await category_elem.inner_text() if category_elem else ""
                    
                    # ì¢Œí‘œ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸)
                    latitude = None
                    longitude = None
                    
                    if platform_id and platform_id != f"kakao_{idx}":
                        try:
                            detail_page = await self.context.new_page()
                            await detail_page.goto(f"https://place.map.kakao.com/{platform_id}", wait_until='domcontentloaded', timeout=10000)
                            await asyncio.sleep(2)
                            
                            # URLì—ì„œ ì¢Œí‘œ ì¶”ì¶œ ì‹œë„ (ì§€ë„ ì´ë™ ì‹œ URLì— ì¢Œí‘œê°€ í¬í•¨ë¨)
                            # ë˜ëŠ” í˜ì´ì§€ ë‚´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
                            coord_script = await detail_page.evaluate('''() => {
                                // og:image ë©”íƒ€ íƒœê·¸ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ (ê°€ì¥ ì‹ ë¢°ì„± ë†’ìŒ)
                                const ogImage = document.querySelector('meta[property="og:image"]');
                                if (ogImage) {
                                    const content = ogImage.getAttribute('content');
                                    // URL í˜•ì‹: ...&m=127.04663583870042,37.54715896195635
                                    const mMatch = content.match(/[&?]m=([0-9.]+),([0-9.]+)/);
                                    if (mMatch) {
                                        return { lat: parseFloat(mMatch[2]), lng: parseFloat(mMatch[1]) };
                                    }
                                }
                                
                                // í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ì¢Œí‘œ íŒ¨í„´ ì°¾ê¸° (ë°±ì—…)
                                const scripts = document.querySelectorAll('script');
                                for (let s of scripts) {
                                    const text = s.textContent || '';
                                    const match = text.match(/"y":"([0-9.]+)","x":"([0-9.]+)"/);
                                    if (match) {
                                        return { lat: parseFloat(match[1]), lng: parseFloat(match[2]) };
                                    }
                                }
                                return null;
                            }''')
                            
                            if coord_script:
                                latitude = coord_script.get('lat')
                                longitude = coord_script.get('lng')
                                print(f"    ğŸ“ ì¢Œí‘œ ì¶”ì¶œ ì„±ê³µ: {latitude}, {longitude}")
                            else:
                                print(f"    âš ï¸ ì¢Œí‘œ ì¶”ì¶œ ì‹¤íŒ¨ (í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ë¨)")
                            
                            await detail_page.close()
                        except Exception as coord_err:
                            print(f"    âš ï¸ ìƒì„¸í˜ì´ì§€ ì¢Œí‘œ ì¶”ì¶œ ì˜¤ë¥˜: {coord_err}")
                    
                    cafe = CafeData(
                        name=name,
                        address=address,
                        category=category,
                        source_platform="KAKAO_MAP",
                        platform_id=platform_id,
                        latitude=latitude,
                        longitude=longitude,
                        status="NEW",
                        raw_data={"detail_url": detail_url}
                    )
                    cafes.append(cafe)
                    
                    await AntiBot.random_scroll(page)
                    
                except Exception as e:
                    print(f"Error parsing cafe {idx}: {e}")
                    continue
        
        finally:
            await page.close()
            await self.close_browser()
        
        return cafes
    
    async def crawl_reviews(self, platform_id: str, since_date: Optional[str] = None) -> List[ReviewData]:
        """ì¦ë¶„ ë¦¬ë·° ìˆ˜ì§‘"""
        await self.init_browser()
        page = await self.context.new_page()
        reviews = []
        
        try:
            detail_url = f"{self.BASE_URL}/{platform_id}"
            await page.goto(detail_url)
            await asyncio.sleep(2)
            
            # ë¦¬ë·° íƒ­ í´ë¦­ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            review_tab = await page.query_selector('.link_evaluation')
            if review_tab:
                await review_tab.click()
                await asyncio.sleep(1)
            
            # ë¦¬ë·° í•­ëª© ì¶”ì¶œ
            review_items = await page.query_selector_all('.list_evaluation > li')
            
            for idx, item in enumerate(review_items):
                try:
                    # ë¦¬ë·° ë‚ ì§œ í™•ì¸ (ì¦ë¶„ ìˆ˜ì§‘)
                    date_elem = await item.query_selector('.time_write')
                    review_date = await date_elem.inner_text() if date_elem else ""
                    
                    # since_date ì´ì „ì´ë©´ ì¤‘ë‹¨
                    if since_date and review_date < since_date:
                        break
                    
                    # ì‘ì„±ì
                    author_elem = await item.query_selector('.link_user')
                    author = await author_elem.inner_text() if author_elem else ""
                    
                    # í‰ì 
                    rating_elem = await item.query_selector('.grade_star')
                    rating_text = await rating_elem.inner_text() if rating_elem else "0"
                    rating = int(rating_text.replace('ì ', '')) if rating_text else None
                    
                    # ë‚´ìš©
                    content_elem = await item.query_selector('.txt_comment')
                    content = await content_elem.inner_text() if content_elem else ""
                    
                    # ì´ë¯¸ì§€
                    img_elem = await item.query_selector('.link_photo img')
                    image_url = await img_elem.get_attribute('src') if img_elem else ""
                    
                    review = ReviewData(
                        cafe_platform_id=platform_id,
                        reviewer_nickname=author,
                        rating=rating,
                        content=content,
                        review_date=review_date,
                        image_url=image_url,
                        source_platform="KAKAO_MAP",
                        platform_review_id=f"{platform_id}_review_{idx}"
                    )
                    reviews.append(review)
                    
                except Exception as e:
                    print(f"Error parsing review {idx}: {e}")
                    continue
        
        finally:
            await page.close()
            await self.close_browser()
        
        return reviews

async def main():
    print("=" * 50)
    print("Kakao Map Crawler Started")
    print("=" * 50)
    
    crawler = KakaoMapCrawler(headless=True)
    # ì˜ˆì‹œ: ì„±ìˆ˜ë™ ì¹´í˜ ê²€ìƒ‰
    cafes = await crawler.discover_new_cafes("ì„±ìˆ˜ë™", limit=5)
    
    all_reviews = []
    
    for cafe in cafes:
        print(f"[NEW] {cafe.name} - {cafe.address} (ID: {cafe.platform_id})")
        # ë¦¬ë·° ìˆ˜ì§‘ (ì²« ë²ˆì§¸ ì¹´í˜ë§Œ ì˜ˆì‹œë¡œ ìˆ˜í–‰í•˜ê±°ë‚˜ ì „ì²´ ìˆ˜í–‰ ê°€ëŠ¥)
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ì²« ë²ˆì§¸ë§Œ
        if cafe == cafes[0]:
             print(f"  Fetching reviews for {cafe.name}...")
             reviews = await crawler.crawl_reviews(cafe.platform_id)
             all_reviews.extend(reviews)
             for review in reviews[:3]:
                 print(f"    [REVIEW] {review.reviewer_nickname}: {review.content[:30]}...")

    # ê²°ê³¼ ì €ì¥
    result = {
        "cafes": [asdict(cafe) for cafe in cafes],
        "reviews": [asdict(review) for review in all_reviews]
    }
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, "kakao_result.json")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nSaved kakao results to {output_path}")

if __name__ == "__main__":
    asyncio.run(main())
